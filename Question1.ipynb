{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import spacy\n",
    "import scispacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Task 1: Extracting text from CSV files\n",
    "def extract_text_from_csvs(zip_file_path):\n",
    "    text_list = []\n",
    "\n",
    "    # Create a temporary folder to extract the files\n",
    "    temp_folder = 'temp_folder'\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "\n",
    "    # Unzip the provided zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temp_folder)\n",
    "\n",
    "    \n",
    "    for filename in os.listdir(temp_folder):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(temp_folder, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            text_column = [col for col in df.columns if 'TEXT' in col][0]\n",
    "            raw_text = ' '.join(df[text_column].tolist())\n",
    "\n",
    "            # Remove non-alphabetic characters and words with less than 4 letters\n",
    "            cleaned_text = ' '.join(re.findall(r'\\b[a-zA-Z]{4,}\\b', raw_text))\n",
    "\n",
    "            text_list.append(cleaned_text)\n",
    "\n",
    "    \n",
    "    with open('combined_text.txt', 'w') as f:\n",
    "        f.write('\\n'.join(text_list))\n",
    "\n",
    "    # Remove the temporary folder after processing\n",
    "    shutil.rmtree(temp_folder)\n",
    "\n",
    "    print('Task 1 complete')\n",
    "\n",
    "\n",
    "zip_file_path = r'C:\\CDU 1st semester\\Software Now\\Assignment 2 (1).zip'\n",
    "\n",
    "\n",
    "extract_text_from_csvs(zip_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2 Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\itonm\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\itonm\\.cache\\huggingface\\hub\\models--alvaroalon2--biobert_diseases_ner. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertModel were not initialized from the model checkpoint at alvaroalon2/biobert_diseases_ner and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load BioBERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"alvaroalon2/biobert_diseases_ner\")\n",
    "model = BertModel.from_pretrained(\"alvaroalon2/biobert_diseases_ner\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load BioBERT model and tokenizer for dmis-lab/biosyn-biobert-bc5cdr-chemical\n",
    "tokenizer_chemical = BertTokenizer.from_pretrained(\"dmis-lab/biosyn-biobert-bc5cdr-chemical\")\n",
    "model_chemical = BertModel.from_pretrained(\"dmis-lab/biosyn-biobert-bc5cdr-chemical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 3\n",
    "# Function to read text in chunks using sliding window\n",
    "def read_text_in_chunks(text_file_path, window_size=512, overlap=100):\n",
    "    with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "        long_text = f.read()\n",
    "        for i in range(0, len(long_text), window_size - overlap):\n",
    "            chunk = long_text[i:i + window_size]\n",
    "            yield chunk\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked\n"
     ]
    }
   ],
   "source": [
    "print ('chunked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "def count_words(text_file):\n",
    "    word_counts = Counter()\n",
    "    for chunk in read_text_in_chunks(text_file):\n",
    "        words = chunk.split()\n",
    "        word_counts.update(words)\n",
    "\n",
    "    top_30_words = word_counts.most_common(30)\n",
    "\n",
    "    with open('top_30_words.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Word', 'Count'])\n",
    "        writer.writerows(top_30_words)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3.1: Top 30 common words and their counts have been stored in \"top_30_words.csv\"\n"
     ]
    }
   ],
   "source": [
    "print('Task 3.1: Top 30 common words and their counts have been stored in \"top_30_words.csv\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read text in chunks using sliding window\n",
    "def read_text_in_chunks(text_file_path, window_size=512, overlap=100):\n",
    "    with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "        long_text = f.read()\n",
    "        for i in range(0, len(long_text), window_size - overlap):\n",
    "            chunk = long_text[i:i + window_size]\n",
    "            yield chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3.2: Top 30 unique tokens and their counts have been stored in \"top_30_unique_tokens.csv\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Function to read text in chunks using sliding window\n",
    "def read_text_in_chunks(text_file_path, window_size=512, overlap=100):\n",
    "    with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "        long_text = f.read()\n",
    "        for i in range(0, len(long_text), window_size - overlap):\n",
    "            chunk = long_text[i:i + window_size]\n",
    "            yield chunk\n",
    "\n",
    "# Task 3.2: Counting unique tokens using Transformers\n",
    "def count_unique_tokens(text_file, model_name=\"dmis-lab/biobert-v1.1\"):\n",
    "    # Using Auto Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Initialize a Counter to keep track of unique tokens\n",
    "    unique_tokens = Counter()\n",
    "\n",
    "    # Iterate over text chunks using the read_text_in_chunks function\n",
    "    for chunk in read_text_in_chunks(text_file):\n",
    "        # Tokenize the chunk using the Auto Tokenizer\n",
    "        tokens = tokenizer.tokenize(chunk)\n",
    "\n",
    "        # Update the Counter with the token occurrences\n",
    "        unique_tokens.update(tokens)\n",
    "\n",
    "    # Get the top 30 most common tokens\n",
    "    top_30_tokens = unique_tokens.most_common(30)\n",
    "\n",
    "    # Write the results to a CSV file\n",
    "    with open('top_30_unique_tokens.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Token', 'Count'])\n",
    "        writer.writerows(top_30_tokens)\n",
    "\n",
    "    print('Task 3.2: Top 30 unique tokens and their counts have been stored in \"top_30_unique_tokens.csv\"')\n",
    "\n",
    "# Call the function with the text file path\n",
    "text_file_path = 'combined_text.txt'\n",
    "count_unique_tokens(text_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task4 \n",
    "#part 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_in_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\itonm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp_sci = spacy.load(\"en_core_sci_sm\")\n",
    "nlp_bc5cdr = spacy.load(\"en_ner_bc5cdr_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_in_chunks(file_path, chunk_size=100000, overlap=20000):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        while True:\n",
    "            chunk = file.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            yield chunk\n",
    "            file.seek(file.tell() - overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = read_text_in_chunks(r\"C:\\CDU 1st semester\\Software Now\\combined_text.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Task 4 part 1\n",
    "# Function for Named Entity Recognition using spaCy\n",
    "\n",
    "# Load spaCy models\n",
    "nlp_sci = spacy.load(\"en_core_sci_sm\")\n",
    "nlp_bc5cdr = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "\n",
    "# Read the content from combined_text.txt in chunks\n",
    "text_chunks = read_text_in_chunks(r\"C:\\CDU 1st semester\\Software Now\\combined_text.txt\")\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_entities(nlp_model, chunk):\n",
    "    entities = []\n",
    "    doc = nlp_model(chunk)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'DISEASE':\n",
    "            entities.append((ent.text, ent.label_))\n",
    "    return entities\n",
    "\n",
    "# Process each chunk using spaCy and extract entities\n",
    "diseases_entities_sci = []\n",
    "diseases_entities_bc5cdr = []\n",
    "for chunk in text_chunks:\n",
    "    diseases_entities_sci.extend(extract_entities(nlp_sci, chunk))\n",
    "    diseases_entities_bc5cdr.extend(extract_entities(nlp_bc5cdr, chunk))\n",
    "\n",
    "# Export entities identified by spaCy to separate CSV files\n",
    "with open('diseases_entities_sci.csv', 'w') as diseases_file_sci:\n",
    "    diseases_file_sci.write('Entity,Label\\n')\n",
    "    for entity in diseases_entities_sci:\n",
    "        diseases_file_sci.write(f'{entity[0]},{entity[1]}\\n')\n",
    "\n",
    "with open('diseases_entities_bc5cdr.csv', 'w') as diseases_file_bc5cdr:\n",
    "    diseases_file_bc5cdr.write('Entity,Label\\n')\n",
    "    for entity in diseases_entities_bc5cdr:\n",
    "        diseases_file_bc5cdr.write(f'{entity[0]},{entity[1]}\\n')\n",
    "\n",
    "print ('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load entities detected by spaCy models\n",
    "spacy_entities_sci = pd.read_csv('diseases_entities_sci.csv')\n",
    "spacy_entities_bc5cdr = pd.read_csv('diseases_entities_bc5cdr.csv')\n",
    "\n",
    "# Load entities detected by BioBERT\n",
    "biobert_entities = pd.read_csv('biobert_entities.csv')\n",
    "\n",
    "# Get total entities for each model\n",
    "total_entities_sci = len(spacy_entities_sci)\n",
    "total_entities_bc5cdr = len(spacy_entities_bc5cdr)\n",
    "total_entities_biobert = len(biobert_entities)\n",
    "\n",
    "# Print total entities detected by each model\n",
    "print(f'Total Entities (spaCy Sci): {total_entities_sci}')\n",
    "print(f'Total Entities (spaCy BC5CDR): {total_entities_bc5cdr}')\n",
    "print(f'Total Entities (BioBERT): {total_entities_biobert}')\n",
    "\n",
    "# Get most common words for each model\n",
    "def get_most_common_words(text):\n",
    "    words = text.split()\n",
    "    word_counts = pd.Series(words).value_counts()\n",
    "    return word_counts[:10]  # Return top 10 most common words\n",
    "\n",
    "most_common_words_sci = get_most_common_words(' '.join(spacy_entities_sci['Entity']))\n",
    "most_common_words_bc5cdr = get_most_common_words(' '.join(spacy_entities_bc5cdr['Entity']))\n",
    "most_common_words_biobert = get_most_common_words(' '.join(biobert_entities['Entity']))\n",
    "\n",
    "# Print most common words for each model\n",
    "print(\"\\nMost Common Words (spaCy Sci):\")\n",
    "print(most_common_words_sci)\n",
    "print(\"\\nMost Common Words (spaCy BC5CDR):\")\n",
    "print(most_common_words_bc5cdr)\n",
    "print(\"\\nMost Common Words (BioBERT):\")\n",
    "print(most_common_words_biobert)\n",
    "\n",
    "# Calculate differences\n",
    "total_difference_sci_bc5cdr = abs(total_entities_sci - total_entities_bc5cdr)\n",
    "total_difference_sci_biobert = abs(total_entities_sci - total_entities_biobert)\n",
    "total_difference_bc5cdr_biobert = abs(total_entities_bc5cdr - total_entities_biobert)\n",
    "\n",
    "print(f\"\\nTotal Difference (spaCy Sci vs spaCy BC5CDR): {total_difference_sci_bc5cdr}\")\n",
    "print(f\"Total Difference (spaCy Sci vs BioBERT): {total_difference_sci_biobert}\")\n",
    "print(f\"Total Difference (spaCy BC5CDR vs BioBERT): {total_difference_bc5cdr_biobert}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
